{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a10c7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingVectorizer + TfidfTransformer time: 4.00 seconds\n",
      "Sparse matrix shape: (141356, 1048576)\n",
      "Sparse matrix memory: 23.02 MB\n",
      "TfidfVectorizer time: 9.40 seconds\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "# Load data\n",
    "data = \"/home/SaiKashyap/ner/translation_data.csv\"\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# Fix the text column preparation\n",
    "# Check if English column contains strings or lists\n",
    "if df['English'].dtype == 'object' and isinstance(df['English'].iloc[0], str):\n",
    "    # If already strings, use as is\n",
    "    df['text'] = df['English']\n",
    "else:\n",
    "    # If lists or other structures, join them\n",
    "    df['text'] = df['English'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "    # Fill NaN values with empty string\n",
    "\n",
    "# Or drop rows with NaN values\n",
    "df = df.dropna(subset=['text'])\n",
    "\n",
    "# Then vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# OPTION 1: FASTEST FOR VERY LARGE DATASETS (>1M rows)\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Generate term frequencies with hashing (memory efficient)\n",
    "hasher = HashingVectorizer(\n",
    "    n_features=2**20,            # 1M features to minimize collisions\n",
    "    ngram_range=(1, 2),          # Include bigrams for better context\n",
    "    alternate_sign=False,        # No negative values for better interpretability\n",
    "    dtype=np.float32             # Use float32 to reduce memory by 50% vs float64\n",
    ")\n",
    "X_counts = hasher.transform(df['text'])\n",
    "\n",
    "# Step 2: Convert counts to TF-IDF\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    sublinear_tf=True,           # Apply 1+log(tf) scaling\n",
    "    use_idf=True,                # Apply inverse document frequency weighting\n",
    "    smooth_idf=True              # Add 1 to document frequencies to prevent division by zero\n",
    ")\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "print(f\"HashingVectorizer + TfidfTransformer time: {time.time() - start_time:.2f} seconds\")\n",
    "print(f\"Sparse matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Sparse matrix memory: {X_tfidf.data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# OPTION 2: RECOMMENDED FOR DATASETS <1M ROWS (cleaner, with feature names)\n",
    "start_time = time.time()\n",
    "\n",
    "# One-step TF-IDF computation\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50_000,         # Limit vocabulary size\n",
    "    sublinear_tf=True,           # Apply 1+log(tf) scaling\n",
    "    ngram_range=(1, 2),          # Include unigrams and bigrams\n",
    "    min_df=2,                    # Ignore terms appearing in less than 2 documents\n",
    "    max_df=0.95,                 # Ignore terms appearing in more than 95% of documents\n",
    "    dtype=np.float32             # Use float32 for memory efficiency\n",
    ")\n",
    "X_tfidf_direct = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "print(f\"TfidfVectorizer time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Example query function\n",
    "def search_similar_documents(query_text, top_n=5):\n",
    "    query_vec = vectorizer.transform([query_text])\n",
    "    # Compute cosine similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarities = cosine_similarity(query_vec, X_tfidf_direct).flatten()\n",
    "    # Get top N indices\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return df.iloc[top_indices][['English', 'text']], similarities[top_indices]\n",
    "\n",
    "# Example usage:\n",
    "# similar_docs, scores = search_similar_documents(\"your query text\", top_n=5)\n",
    "# print(similar_docs)\n",
    "# print(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57d552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Process dataframe in chunks to reduce memory pressure\n",
    "chunk_size = 10000\n",
    "X_list = []\n",
    "\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    chunk = df.iloc[i:i+chunk_size]\n",
    "    X_chunk = vectorizer.transform(chunk['text'])\n",
    "    X_list.append(X_chunk)\n",
    "\n",
    "from scipy import sparse\n",
    "X_tfidf = sparse.vstack(X_list)\n",
    "\n",
    "# 2. Use multiprocessing for parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    return vectorizer.transform(chunk)\n",
    "\n",
    "chunks = [df['text'][i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "X_list = Parallel(n_jobs=-1)(delayed(process_chunk)(chunk) for chunk in chunks)\n",
    "X_tfidf = sparse.vstack(X_list)\n",
    "\n",
    "# 3. Save the sparse matrix efficiently for later use\n",
    "from scipy.sparse import save_npz\n",
    "save_npz('tfidf_matrix.npz', X_tfidf)\n",
    "\n",
    "# 4. Load the matrix when needed\n",
    "from scipy.sparse import load_npz\n",
    "X_tfidf = load_npz('tfidf_matrix.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873c56a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "126227                      The lute of Narada is Mahati.   \n",
      "136042                              This is a rishi/sage.   \n",
      "67658   There Sthulaksha, Shavalaksha, Kanwa, Medhatit...   \n",
      "110566                            Narada wandered in sky.   \n",
      "133350           Saint Narada visits hermitage of Valmiki   \n",
      "\n",
      "                                                     text  \n",
      "126227                      The lute of Narada is Mahati.  \n",
      "136042                              This is a rishi/sage.  \n",
      "67658   There Sthulaksha, Shavalaksha, Kanwa, Medhatit...  \n",
      "110566                            Narada wandered in sky.  \n",
      "133350           Saint Narada visits hermitage of Valmiki  \n",
      "[0.42471656 0.3312608  0.32149124 0.30741057 0.28697023]\n"
     ]
    }
   ],
   "source": [
    " similar_docs, scores = search_similar_documents(\"Narada rishi\", top_n=5)\n",
    "print(similar_docs)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39c6602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 141356 documents\n",
      "\n",
      "--- SCIKIT-LEARN IMPLEMENTATION ---\n",
      "HashingVectorizer + TfidfTransformer time: 4.05 seconds\n",
      "Sparse matrix shape: (141356, 262144)\n",
      "Sparse matrix memory: 23.02 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-20 16:01:24,089 [ERROR][handler]: RPC error: [__internal_register], <MilvusException: (code=1, message=Incorrect port or sdk is incompatible with server, please check your port or downgrade your sdk or upgrade your server)>, <Time:{'RPC start': '2025-03-20 16:01:24.088293', 'RPC error': '2025-03-20 16:01:24.089565'}> (decorators.py:140)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer time: 9.37 seconds\n",
      "\n",
      "Scikit-learn search time: 0.1321 seconds\n",
      "\n",
      "--- MILVUS VECTOR DATABASE IMPLEMENTATION ---\n",
      "\n",
      "Error connecting to Milvus: <MilvusException: (code=1, message=Incorrect port or sdk is incompatible with server, please check your port or downgrade your sdk or upgrade your server)>\n",
      "To use Milvus, make sure it's properly installed and running. You can install it via:\n",
      "  pip install pymilvus\n",
      "And start Milvus using Docker with:\n",
      "  docker run -d --name milvus -p 19530:19530 -p 19121:19121 milvusdb/milvus:latest\n",
      "\n",
      "--- SCALING CONSIDERATIONS ---\n",
      "1. For datasets < 1M documents: TfidfVectorizer + Scikit-learn is sufficient\n",
      "2. For 1M-10M documents: Milvus with IVF_FLAT provides better search latency\n",
      "3. For >10M documents: Consider these optimizations:\n",
      "   - Use HashingVectorizer to reduce memory usage\n",
      "   - Process in batches of 10K-100K documents\n",
      "   - Use HNSW index in Milvus for better search performance\n",
      "   - Consider distributed deployment with Milvus shards\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# For Milvus integration\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "# Load data\n",
    "data = \"/home/SaiKashyap/ner/translation_data.csv\"\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# Fix the text column preparation\n",
    "if df['English'].dtype == 'object' and isinstance(df['English'].iloc[0], str):\n",
    "    df['text'] = df['English']\n",
    "else:\n",
    "    df['text'] = df['English'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna(subset=['text'])\n",
    "print(f\"Dataset size: {len(df)} documents\")\n",
    "\n",
    "# Reset index to use as document IDs\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#########################\n",
    "# PART 1: SCIKIT-LEARN TFIDF IMPLEMENTATION\n",
    "#########################\n",
    "\n",
    "print(\"\\n--- SCIKIT-LEARN IMPLEMENTATION ---\")\n",
    "\n",
    "# OPTION 1: HashingVectorizer + TfidfTransformer (memory efficient)\n",
    "start_time = time.time()\n",
    "\n",
    "hasher = HashingVectorizer(\n",
    "    n_features=2**18,            # 262K features (reduced for comparison fairness)\n",
    "    ngram_range=(1, 2),          \n",
    "    alternate_sign=False,        \n",
    "    dtype=np.float32             \n",
    ")\n",
    "X_counts = hasher.transform(df['text'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    sublinear_tf=True,           \n",
    "    use_idf=True,                \n",
    "    smooth_idf=True              \n",
    ")\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "hashing_time = time.time() - start_time\n",
    "print(f\"HashingVectorizer + TfidfTransformer time: {hashing_time:.2f} seconds\")\n",
    "print(f\"Sparse matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Sparse matrix memory: {X_tfidf.data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# OPTION 2: One-step TfidfVectorizer (with feature names)\n",
    "start_time = time.time()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50_000,         \n",
    "    sublinear_tf=True,           \n",
    "    ngram_range=(1, 2),          \n",
    "    min_df=2,                    \n",
    "    max_df=0.95,                 \n",
    "    dtype=np.float32             \n",
    ")\n",
    "X_tfidf_direct = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "tfidf_time = time.time() - start_time\n",
    "print(f\"TfidfVectorizer time: {tfidf_time:.2f} seconds\")\n",
    "\n",
    "# Test scikit-learn search speed\n",
    "def sklearn_search(query_text, top_n=5):\n",
    "    start_time = time.time()\n",
    "    query_vec = vectorizer.transform([query_text])\n",
    "    similarities = cosine_similarity(query_vec, X_tfidf_direct).flatten()\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    search_time = time.time() - start_time\n",
    "    return df.iloc[top_indices][['English', 'text']], similarities[top_indices], search_time\n",
    "\n",
    "# Test search with a sample query\n",
    "test_query = \"Narada Maharishi\"\n",
    "results, scores, search_time = sklearn_search(test_query, top_n=5)\n",
    "print(f\"\\nScikit-learn search time: {search_time:.4f} seconds\")\n",
    "\n",
    "#########################\n",
    "# PART 2: MILVUS VECTOR DATABASE IMPLEMENTATION\n",
    "#########################\n",
    "\n",
    "print(\"\\n--- MILVUS VECTOR DATABASE IMPLEMENTATION ---\")\n",
    "\n",
    "try:\n",
    "    # Connect to Milvus (make sure Milvus is running)\n",
    "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "    \n",
    "    # Define collection schema\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=X_tfidf_direct.shape[1])\n",
    "    ]\n",
    "    schema = CollectionSchema(fields, \"TF-IDF vectors for text search\")\n",
    "    \n",
    "    # Create or recreate collection\n",
    "    collection_name = \"tfidf_vectors\"\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    collection = Collection(collection_name, schema)\n",
    "    \n",
    "    # Insert vectors into Milvus\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert sparse matrix to dense for Milvus\n",
    "    # Note: For large datasets, process in batches to avoid memory issues\n",
    "    batch_size = 1000\n",
    "    total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i+1) * batch_size, len(df))\n",
    "        \n",
    "        # Extract batch of vectors and convert to dense\n",
    "        batch_vectors = X_tfidf_direct[start_idx:end_idx].toarray()\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        entities = [\n",
    "            # Document IDs\n",
    "            list(range(start_idx, end_idx)),\n",
    "            # TF-IDF vectors\n",
    "            batch_vectors.tolist()\n",
    "        ]\n",
    "        \n",
    "        # Insert into collection\n",
    "        collection.insert(entities)\n",
    "        \n",
    "        if (i + 1) % 10 == 0 or (i + 1) == total_batches:\n",
    "            print(f\"Processed batch {i+1}/{total_batches}\")\n",
    "    \n",
    "    # Create IVF_FLAT index (efficient for medium-sized datasets)\n",
    "    index_params = {\n",
    "        \"metric_type\": \"IP\",  # Inner product for cosine similarity\n",
    "        \"index_type\": \"IVF_FLAT\",\n",
    "        \"params\": {\"nlist\": 128}  # Number of clusters\n",
    "    }\n",
    "    \n",
    "    print(\"Building Milvus index...\")\n",
    "    collection.create_index(\"embedding\", index_params)\n",
    "    \n",
    "    # Load collection into memory for search\n",
    "    collection.load()\n",
    "    \n",
    "    milvus_build_time = time.time() - start_time\n",
    "    print(f\"Milvus index build time: {milvus_build_time:.2f} seconds\")\n",
    "    \n",
    "    # Define Milvus search function\n",
    "    def milvus_search(query_text, top_n=5):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert query to vector\n",
    "        query_vec = vectorizer.transform([query_text]).toarray()\n",
    "        \n",
    "        # Search parameters\n",
    "        search_params = {\n",
    "            \"metric_type\": \"IP\",  # Inner product for cosine similarity\n",
    "            \"params\": {\"nprobe\": 16}  # Number of clusters to search\n",
    "        }\n",
    "        \n",
    "        # Perform search\n",
    "        results = collection.search(\n",
    "            data=query_vec.tolist(),\n",
    "            anns_field=\"embedding\",\n",
    "            param=search_params,\n",
    "            limit=top_n,\n",
    "            output_fields=[]\n",
    "        )\n",
    "        \n",
    "        # Extract results\n",
    "        doc_ids = [hit.id for hit in results[0]]\n",
    "        scores = [hit.score for hit in results[0]]\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        return df.iloc[doc_ids][['English', 'text']], scores, search_time\n",
    "    \n",
    "    # Test Milvus search with the same query\n",
    "    milvus_results, milvus_scores, milvus_search_time = milvus_search(test_query, top_n=5)\n",
    "    print(f\"Milvus search time: {milvus_search_time:.4f} seconds\")\n",
    "    \n",
    "    # Comparison of both methods\n",
    "    print(\"\\n--- PERFORMANCE COMPARISON ---\")\n",
    "    print(f\"{'Method':<20} {'Build Time (s)':<15} {'Search Time (s)':<15}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    print(f\"{'Scikit-learn TF-IDF':<20} {tfidf_time:<15.2f} {search_time:<15.6f}\")\n",
    "    print(f\"{'Milvus Vector DB':<20} {milvus_build_time:<15.2f} {milvus_search_time:<15.6f}\")\n",
    "    print(f\"{'Speedup Ratio':<20} {'N/A':<15} {search_time/milvus_search_time if milvus_search_time > 0 else 'N/A':<15.2f}\")\n",
    "    \n",
    "    # Optional: Show detailed results comparison\n",
    "    print(\"\\n--- SEARCH RESULTS COMPARISON ---\")\n",
    "    print(\"\\nScikit-learn top 3 results:\")\n",
    "    print(results[['English']].head(3))\n",
    "    \n",
    "    print(\"\\nMilvus top 3 results:\")\n",
    "    print(milvus_results[['English']].head(3))\n",
    "    \n",
    "    # Close Milvus connection\n",
    "    collection.release()\n",
    "    connections.disconnect(\"default\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError connecting to Milvus: {e}\")\n",
    "    print(\"To use Milvus, make sure it's properly installed and running. You can install it via:\")\n",
    "    print(\"  pip install pymilvus\")\n",
    "    print(\"And start Milvus using Docker with:\")\n",
    "    print(\"  docker run -d --name milvus -p 19530:19530 -p 19121:19121 milvusdb/milvus:latest\")\n",
    "\n",
    "#########################\n",
    "# PART 3: SCALING CONSIDERATIONS\n",
    "#########################\n",
    "\n",
    "print(\"\\n--- SCALING CONSIDERATIONS ---\")\n",
    "print(\"1. For datasets < 1M documents: TfidfVectorizer + Scikit-learn is sufficient\")\n",
    "print(\"2. For 1M-10M documents: Milvus with IVF_FLAT provides better search latency\")\n",
    "print(\"3. For >10M documents: Consider these optimizations:\")\n",
    "print(\"   - Use HashingVectorizer to reduce memory usage\")\n",
    "print(\"   - Process in batches of 10K-100K documents\")\n",
    "print(\"   - Use HNSW index in Milvus for better search performance\")\n",
    "print(\"   - Consider distributed deployment with Milvus shards\")\n",
    "\n",
    "# Batch processing example for very large datasets\n",
    "def process_large_dataset(file_path, batch_size=10000):\n",
    "    # Initialize vectorizer\n",
    "    vectorizer = HashingVectorizer(\n",
    "        n_features=2**20, \n",
    "        ngram_range=(1, 2), \n",
    "        alternate_sign=False\n",
    "    )\n",
    "    \n",
    "    # Process in batches\n",
    "    for chunk in pd.read_csv(file_path, chunksize=batch_size):\n",
    "        # Process chunk\n",
    "        X_chunk = vectorizer.transform(chunk['text_column'].fillna(''))\n",
    "        \n",
    "        # Here you would insert into Milvus or other storage\n",
    "        # collection.insert([chunk.index.tolist(), X_chunk.toarray().tolist()])\n",
    "        \n",
    "    print(f\"Processed dataset in batches of {batch_size}\")\n",
    "\n",
    "# Note: Uncomment to process a very large file in batches\n",
    "# process_large_dataset(\"/path/to/large/file.csv\", batch_size=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225c328d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages/cupy/_environment.py:541: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda11x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU VERSION (scikit-learn) ===\n",
      "CPU data loading time: 0.7754 seconds\n",
      "CPU HashingVectorizer + TfidfTransformer time: 3.3145 seconds\n",
      "Sparse matrix shape: (141356, 262144)\n",
      "Sparse matrix memory: 23.02 MB\n",
      "CPU TfidfVectorizer time: 7.2097 seconds\n",
      "CPU search time: 0.110851 seconds\n",
      "\n",
      "=== GPU VERSION (cuDF/cuML) ===\n",
      "GPU data loading time: 4.4059 seconds\n",
      "GPU HashingVectorizer + TfidfTransformer time: 0.5421 seconds\n",
      "GPU matrix shape: (141356, 262144)\n",
      "GPU matrix memory: N/A MB\n",
      "GPU TfidfVectorizer time: 0.2742 seconds\n",
      "\n",
      "GPU search time: 0.315697 seconds\n",
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "Operation                 CPU Time (s)    GPU Time (s)    Speedup   \n",
      "-----------------------------------------------------------------\n",
      "Data Loading              0.7754          4.4059          0.18      x\n",
      "Hashing + TF-IDF          3.3145          0.5421          6.11      x\n",
      "TfidfVectorizer           7.2097          0.2742          26.29     x\n",
      "Search                    0.110851        0.315697        0.35      x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages/numba_cuda/numba/cuda/dispatcher.py:579: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# GPU libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.feature_extraction.text import HashingVectorizer as cuHashingVectorizer\n",
    "from cuml.feature_extraction.text import TfidfTransformer as cuTfidfTransformer\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n",
    "\n",
    "# Load data\n",
    "data = \"/home/SaiKashyap/ner/translation_data.csv\"\n",
    "\n",
    "# --------- CPU VERSION (ORIGINAL) ---------\n",
    "print(\"=== CPU VERSION (scikit-learn) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "# Fix the text column preparation\n",
    "if df['English'].dtype == 'object' and isinstance(df['English'].iloc[0], str):\n",
    "    df['text'] = df['English']\n",
    "else:\n",
    "    df['text'] = df['English'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna(subset=['text'])\n",
    "cpu_load_time = time.time() - start_time\n",
    "print(f\"CPU data loading time: {cpu_load_time:.4f} seconds\")\n",
    "\n",
    "# OPTION 1: HashingVectorizer + TfidfTransformer\n",
    "start_time = time.time()\n",
    "hasher = HashingVectorizer(\n",
    "    n_features=2**18,\n",
    "    ngram_range=(1, 2),\n",
    "    alternate_sign=False,\n",
    "    dtype=np.float32\n",
    ")\n",
    "X_counts = hasher.transform(df['text'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    ")\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "cpu_hashing_time = time.time() - start_time\n",
    "print(f\"CPU HashingVectorizer + TfidfTransformer time: {cpu_hashing_time:.4f} seconds\")\n",
    "print(f\"Sparse matrix shape: {X_tfidf.shape}\")\n",
    "print(f\"Sparse matrix memory: {X_tfidf.data.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# OPTION 2: One-step TfidfVectorizer\n",
    "start_time = time.time()\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=50_000,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    dtype=np.float32\n",
    ")\n",
    "X_tfidf_direct = vectorizer.fit_transform(df['text'])\n",
    "cpu_tfidf_time = time.time() - start_time\n",
    "print(f\"CPU TfidfVectorizer time: {cpu_tfidf_time:.4f} seconds\")\n",
    "\n",
    "# Test search time\n",
    "def cpu_search(query_text, top_n=5):\n",
    "    start_time = time.time()\n",
    "    query_vec = vectorizer.transform([query_text])\n",
    "    similarities = cosine_similarity(query_vec, X_tfidf_direct).flatten()\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    search_time = time.time() - start_time\n",
    "    return df.iloc[top_indices][['English', 'text']], similarities[top_indices], search_time\n",
    "\n",
    "test_query = \"machine learning algorithms\"\n",
    "cpu_results, cpu_scores, cpu_search_time = cpu_search(test_query, top_n=5)\n",
    "print(f\"CPU search time: {cpu_search_time:.6f} seconds\")\n",
    "\n",
    "# --------- GPU VERSION (cuDF/cuML) ---------\n",
    "print(\"\\n=== GPU VERSION (cuDF/cuML) ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Load data with cuDF\n",
    "gpu_df = cudf.read_csv(data)\n",
    "\n",
    "# Fix the text column preparation for GPU\n",
    "if gpu_df['English'].dtype == 'object':\n",
    "    gpu_df['text'] = gpu_df['English']\n",
    "else:\n",
    "    # Convert to pandas for complex operations, then back to cuDF\n",
    "    temp_df = gpu_df.to_pandas()\n",
    "    temp_df['text'] = temp_df['English'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "    gpu_df = cudf.DataFrame.from_pandas(temp_df)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "gpu_df = gpu_df.dropna(subset=['text'])\n",
    "gpu_load_time = time.time() - start_time\n",
    "print(f\"GPU data loading time: {gpu_load_time:.4f} seconds\")\n",
    "\n",
    "# OPTION 1: cuML HashingVectorizer + TfidfTransformer\n",
    "start_time = time.time()\n",
    "cu_hasher = cuHashingVectorizer(\n",
    "    n_features=2**18,\n",
    "    ngram_range=(1, 2),\n",
    "    alternate_sign=False\n",
    ")\n",
    "X_cu_counts = cu_hasher.transform(gpu_df['text'])\n",
    "\n",
    "cu_tfidf_transformer = cuTfidfTransformer(\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    "    smooth_idf=True\n",
    ")\n",
    "X_cu_tfidf = cu_tfidf_transformer.fit_transform(X_cu_counts)\n",
    "gpu_hashing_time = time.time() - start_time\n",
    "print(f\"GPU HashingVectorizer + TfidfTransformer time: {gpu_hashing_time:.4f} seconds\")\n",
    "print(f\"GPU matrix shape: {X_cu_tfidf.shape}\")\n",
    "print(f\"GPU matrix memory: {X_cu_tfidf.nbytes / 1024 / 1024 if hasattr(X_cu_tfidf, 'nbytes') else 'N/A'} MB\")\n",
    "\n",
    "# OPTION 2: One-step cuML TfidfVectorizer\n",
    "start_time = time.time()\n",
    "cu_vectorizer = cuTfidfVectorizer(\n",
    "    max_features=50_000,\n",
    "    sublinear_tf=True,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "X_cu_tfidf_direct = cu_vectorizer.fit_transform(gpu_df['text'])\n",
    "gpu_tfidf_time = time.time() - start_time\n",
    "print(f\"GPU TfidfVectorizer time: {gpu_tfidf_time:.4f} seconds\")\n",
    "\n",
    "# Modified GPU Search Implementation with CuPy Compatibility\n",
    "\n",
    "\n",
    "def gpu_search(query_text, top_n=5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # --------------------------\n",
    "        # Step 1: Check CuPy Installation\n",
    "        # --------------------------\n",
    "        try:\n",
    "            import cupy as cp\n",
    "            from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix\n",
    "        except ImportError:\n",
    "            raise RuntimeError(\"CuPy not installed. Install with: conda install -c conda-forge cupy\")\n",
    "\n",
    "        # --------------------------\n",
    "        # Step 2: Query Vectorization\n",
    "        # --------------------------\n",
    "        query_series = cudf.Series([query_text])\n",
    "        query_vec = cu_vectorizer.transform(query_series).astype(cp.float32)\n",
    "        \n",
    "        # --------------------------\n",
    "        # Step 3: Sparse Matrix Conversion\n",
    "        # --------------------------\n",
    "        # Convert to CSR format for cuSPARSE compatibility\n",
    "        query_csr = query_vec.tocsr()\n",
    "        corpus_csr = X_cu_tfidf_direct.tocsr()\n",
    "        \n",
    "        # Convert to CuPy CSR matrices\n",
    "        query_gpu = cp_csr_matrix(query_csr)\n",
    "        corpus_gpu = cp_csr_matrix(corpus_csr)\n",
    "        \n",
    "        # --------------------------\n",
    "        # Step 4: Sparse Matrix Multiplication\n",
    "        # --------------------------\n",
    "        similarities = (query_gpu * corpus_gpu.T).todense().ravel()\n",
    "        \n",
    "        # --------------------------\n",
    "        # Step 5: Results Processing\n",
    "        # --------------------------\n",
    "        top_indices = cp.argsort(-similarities)[:top_n].get()\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        return (\n",
    "            gpu_df.iloc[top_indices][['English', 'text']].to_pandas(),\n",
    "            similarities[top_indices].get(),\n",
    "            search_time\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"GPU search error: {str(e)}\")\n",
    "        print(\"Falling back to CPU implementation\")\n",
    "        return cpu_search(query_text, top_n)\n",
    "\n",
    "    \n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.preprocessing import normalize\n",
    "\n",
    "def gpu_search(query_text, top_n=5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Normalize vectors during preprocessing\n",
    "        X_normalized = normalize(X_cu_tfidf_direct, norm='l2')\n",
    "        \n",
    "        # Create GPU-optimized index\n",
    "        nn_model = NearestNeighbors(n_neighbors=top_n, metric='cosine')\n",
    "        nn_model.fit(X_normalized)\n",
    "        \n",
    "        # Vectorize query\n",
    "        query_vec = normalize(cu_vectorizer.transform(cudf.Series([query_text])), norm='l2')\n",
    "        \n",
    "        # GPU-accelerated similarity search\n",
    "        distances, indices = nn_model.kneighbors(query_vec)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        return (\n",
    "            gpu_df.iloc[indices[0].get()][['English', 'text']].to_pandas(),\n",
    "            1 - distances[0].get(),  # Convert cosine distance to similarity\n",
    "            search_time\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"GPU search error: {str(e)}\")\n",
    "        return cpu_search(query_text, top_n)\n",
    "\n",
    "    \n",
    "# Optimized CPU Fallback Implementation\n",
    "def cpu_search(query_text, top_n=5):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use sparse matrix operations\n",
    "    query_vec = vectorizer.transform([query_text])\n",
    "    similarities = (query_vec * X_tfidf_direct.T).toarray().flatten()\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    search_time = time.time() - start_time\n",
    "    return (\n",
    "        df.iloc[top_indices][['English', 'text']],\n",
    "        similarities[top_indices],\n",
    "        search_time\n",
    "    )\n",
    "\n",
    "# Add this before the performance comparison section\n",
    "# Execute GPU search\n",
    "gpu_results, gpu_scores, gpu_search_time = gpu_search(test_query, top_n=5)\n",
    "print(f\"\\nGPU search time: {gpu_search_time:.6f} seconds\")\n",
    "\n",
    "# Then keep the existing performance comparison block\n",
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"{'Operation':<25} {'CPU Time (s)':<15} {'GPU Time (s)':<15} {'Speedup':<10}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'Data Loading':<25} {cpu_load_time:<15.4f} {gpu_load_time:<15.4f} {cpu_load_time/gpu_load_time if gpu_load_time > 0 else 'N/A':<10.2f}x\")\n",
    "print(f\"{'Hashing + TF-IDF':<25} {cpu_hashing_time:<15.4f} {gpu_hashing_time:<15.4f} {cpu_hashing_time/gpu_hashing_time if gpu_hashing_time > 0 else 'N/A':<10.2f}x\")\n",
    "print(f\"{'TfidfVectorizer':<25} {cpu_tfidf_time:<15.4f} {gpu_tfidf_time:<15.4f} {cpu_tfidf_time/gpu_tfidf_time if gpu_tfidf_time > 0 else 'N/A':<10.2f}x\")\n",
    "print(f\"{'Search':<25} {cpu_search_time:<15.6f} {gpu_search_time:<15.6f} {cpu_search_time/gpu_search_time if gpu_search_time > 0 else 'N/A':<10.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4fab545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymilvus==2.0.2\n",
      "  Downloading pymilvus-2.0.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio==1.37.1 (from pymilvus==2.0.2)\n",
      "  Downloading grpcio-1.37.1.tar.gz (21.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting grpcio-tools==1.37.1 (from pymilvus==2.0.2)\n",
      "  Downloading grpcio-tools-1.37.1.tar.gz (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ujson<=5.1.0,>=2.0.0 (from pymilvus==2.0.2)\n",
      "  Downloading ujson-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting mmh3<=3.0.0,>=2.0 (from pymilvus==2.0.2)\n",
      "  Downloading mmh3-3.0.0.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.4 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from pymilvus==2.0.2) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from grpcio==1.37.1->pymilvus==2.0.2) (1.16.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.5.0.post1 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from grpcio-tools==1.37.1->pymilvus==2.0.2) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from grpcio-tools==1.37.1->pymilvus==2.0.2) (76.1.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus==2.0.2) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus==2.0.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus==2.0.2) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus==2.0.2) (2024.2)\n",
      "Downloading pymilvus-2.0.2-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.6/119.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ujson-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: grpcio, grpcio-tools, mmh3\n",
      "  Building wheel for grpcio (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpcio: filename=grpcio-1.37.1-cp310-cp310-linux_x86_64.whl size=3722690 sha256=f4f1efe8f1bb5cc1ed8b8600b11d5bfc181835364ebb17ba7f3b0564014483b3\n",
      "  Stored in directory: /home/SaiKashyap/.cache/pip/wheels/1c/3b/c7/1c579fb0666f67f9b7bd2fab0a1aefd0bc6cc424f7c7ba63a7\n",
      "  Building wheel for grpcio-tools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for grpcio-tools: filename=grpcio_tools-1.37.1-cp310-cp310-linux_x86_64.whl size=2306203 sha256=d103d540cb992bf533e6fed9712e86c90c5ce80bf64f35b202433f7cb7a450fe\n",
      "  Stored in directory: /home/SaiKashyap/.cache/pip/wheels/f6/4d/72/9bf13b99139d23174915bf7194ae0c6eb0e783a9b7e6814972\n",
      "  Building wheel for mmh3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for mmh3: filename=mmh3-3.0.0-cp310-cp310-linux_x86_64.whl size=10813 sha256=e469a7f5de8f13eba13cb5e215cad7ebe5febe99a6bf35f19ed367caa71d50c8\n",
      "  Stored in directory: /home/SaiKashyap/.cache/pip/wheels/01/5e/f4/ea9f16c5ab10dc5596f497b829ec654c7e48f4f5d1a5c4c97a\n",
      "Successfully built grpcio grpcio-tools mmh3\n",
      "Installing collected packages: mmh3, ujson, grpcio, grpcio-tools, pymilvus\n",
      "  Attempting uninstall: ujson\n",
      "    Found existing installation: ujson 5.10.0\n",
      "    Uninstalling ujson-5.10.0:\n",
      "      Successfully uninstalled ujson-5.10.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.62.1\n",
      "    Uninstalling grpcio-1.62.1:\n",
      "      Successfully uninstalled grpcio-1.62.1\n",
      "  Attempting uninstall: pymilvus\n",
      "    Found existing installation: pymilvus 2.5.5\n",
      "    Uninstalling pymilvus-2.5.5:\n",
      "      Successfully uninstalled pymilvus-2.5.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.16.2 requires grpcio>=1.48.2, but you have grpcio 1.37.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.37.1 grpcio-tools-1.37.1 mmh3-3.0.0 pymilvus-2.0.2 ujson-5.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymilvus==2.0.2\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2bd3aef",
   "metadata": {},
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "print(connections.get_connection().server_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff90d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-llama_factory] *",
   "language": "python",
   "name": "conda-env-.conda-llama_factory-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
