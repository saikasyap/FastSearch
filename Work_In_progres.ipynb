{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33ab2d6",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354ba08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/SaiKashyap/.conda/envs/llama_factory/lib/python3.10/site-packages/cupy/_environment.py:541: UserWarning: \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  CuPy may not function correctly because multiple CuPy packages are installed\n",
      "  in your environment:\n",
      "\n",
      "    cupy, cupy-cuda11x\n",
      "\n",
      "  Follow these steps to resolve this issue:\n",
      "\n",
      "    1. For all packages listed above, run the following command to remove all\n",
      "       existing CuPy installations:\n",
      "\n",
      "         $ pip uninstall <package_name>\n",
      "\n",
      "      If you previously installed CuPy via conda, also run the following:\n",
      "\n",
      "         $ conda uninstall cupy\n",
      "\n",
      "    2. Install the appropriate CuPy package.\n",
      "       Refer to the Installation Guide for detailed instructions.\n",
      "\n",
      "         https://docs.cupy.dev/en/stable/install.html\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  warnings.warn(f'''\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# GPU Libraries\n",
    "import cudf\n",
    "import cupy as cp\n",
    "from cuml.feature_extraction.text import HashingVectorizer as cuHashingVectorizer\n",
    "from cuml.feature_extraction.text import TfidfTransformer as cuTfidfTransformer\n",
    "from cuml.feature_extraction.text import TfidfVectorizer as cuTfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bda7f6",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3248a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CPU VERSION (scikit-learn) ===\n"
     ]
    }
   ],
   "source": [
    "data = \"/home/SaiKashyap/ner/translation_data.csv\"\n",
    "df = pd.read_csv(data)\n",
    "gpu_df = cudf.read_csv(data)\n",
    "print(\"=== CPU VERSION (scikit-learn) ===\")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dc919",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981a8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['English'].dtype == 'object' and isinstance(df['English'].iloc[0], str):\n",
    "    df['text'] = df['English']\n",
    "else:\n",
    "    df['text'] = df['English'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "\n",
    "df = df.dropna(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_df['text'] = gpu_df['English'].astype(str)\n",
    "gpu_df = gpu_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dec1e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU data loading time: 0.1783 seconds\n"
     ]
    }
   ],
   "source": [
    "cpu_load_time = time.time() - start_time\n",
    "print(f\"CPU data loading time: {cpu_load_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53327e7",
   "metadata": {},
   "source": [
    "# CPU Version\n",
    "HashingVectorizer: Converts text into high-dimensional vectors (bag-of-words)\n",
    "\n",
    "TfidfTransformer: Converts these counts into TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb89912",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher = HashingVectorizer(n_features=2**18, ngram_range=(1, 2), alternate_sign=False, dtype=np.float32)\n",
    "X_counts = hasher.transform(df['text'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(sublinear_tf=True, use_idf=True, smooth_idf=True)\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23fac0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=50_000, sublinear_tf=True, ngram_range=(1, 2), min_df=2, max_df=0.95, dtype=np.float32)\n",
    "X_tfidf_direct = vectorizer.fit_transform(df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0cba30",
   "metadata": {},
   "source": [
    "# GPU Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "959e9901",
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_hasher = cuHashingVectorizer(n_features=2**18, ngram_range=(1, 2), alternate_sign=False)\n",
    "X_cu_counts = cu_hasher.transform(gpu_df['text'])\n",
    "\n",
    "cu_tfidf_transformer = cuTfidfTransformer(sublinear_tf=True, use_idf=True, smooth_idf=True)\n",
    "X_cu_tfidf = cu_tfidf_transformer.fit_transform(X_cu_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f81b86",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /home/SaiKashyap/.conda/envs/llama_factory/include/rmm/mr/device/cuda_memory_resource.hpp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cu_vectorizer \u001b[38;5;241m=\u001b[39m cuTfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000\u001b[39m, sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X_cu_tfidf_direct \u001b[38;5;241m=\u001b[39m \u001b[43mcu_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpu_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cuml/feature_extraction/_tfidf_vectorizer.py:276\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    260\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn vocabulary and idf, return document-term matrix.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    This is equivalent to fit followed by transform, but more efficiently\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    implemented.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cuml/feature_extraction/_vectorizers.py:596\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    593\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess(raw_documents)\n\u001b[1;32m    594\u001b[0m n_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs)\n\u001b[0;32m--> 596\u001b[0m tokenized_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_tokenized_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fixed_vocabulary:\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cuml/feature_extraction/_vectorizers.py:244\u001b[0m, in \u001b[0;36m_VectorizerMixin._create_tokenized_df\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    241\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(docs), dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    242\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m Series(doc_id)\n\u001b[0;32m--> 244\u001b[0m tokenized_df_ls \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_ngrams(docs, n, doc_id) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_n, max_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m ]\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m docs\n\u001b[1;32m    248\u001b[0m tokenized_df \u001b[38;5;241m=\u001b[39m cudf\u001b[38;5;241m.\u001b[39mconcat(tokenized_df_ls)\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cuml/feature_extraction/_vectorizers.py:245\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    241\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(docs), dtype\u001b[38;5;241m=\u001b[39mcp\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m    242\u001b[0m doc_id \u001b[38;5;241m=\u001b[39m Series(doc_id)\n\u001b[1;32m    244\u001b[0m tokenized_df_ls \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_ngrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(min_n, max_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    246\u001b[0m ]\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m docs\n\u001b[1;32m    248\u001b[0m tokenized_df \u001b[38;5;241m=\u001b[39m cudf\u001b[38;5;241m.\u001b[39mconcat(tokenized_df_ls)\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cuml/feature_extraction/_vectorizers.py:212\u001b[0m, in \u001b[0;36m_VectorizerMixin.get_ngrams\u001b[0;34m(self, str_series, ngram_size, doc_id_sr)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    211\u001b[0m     token_count_sr \u001b[38;5;241m=\u001b[39m str_series\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mtoken_count(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter)\n\u001b[0;32m--> 212\u001b[0m     ngram_sr \u001b[38;5;241m=\u001b[39m \u001b[43mstr_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngrams_tokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelimiter\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# formula to count ngrams given number of tokens x per doc: x-(n-1)\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     ngram_count \u001b[38;5;241m=\u001b[39m token_count_sr \u001b[38;5;241m-\u001b[39m (ngram_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cudf/core/column/string.py:5065\u001b[0m, in \u001b[0;36mStringMethods.ngrams_tokenize\u001b[0;34m(self, n, delimiter, separator)\u001b[0m\n\u001b[1;32m   5062\u001b[0m delim \u001b[38;5;241m=\u001b[39m _massage_string_arg(delimiter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5063\u001b[0m sep \u001b[38;5;241m=\u001b[39m _massage_string_arg(separator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseparator\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_or_inplace(\n\u001b[0;32m-> 5065\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_column\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngrams_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   5066\u001b[0m     retain_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5067\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama_factory/lib/python3.10/site-packages/cudf/core/column/string.py:6221\u001b[0m, in \u001b[0;36mStringColumn.ngrams_tokenize\u001b[0;34m(self, ngrams, delimiter, separator)\u001b[0m\n\u001b[1;32m   6213\u001b[0m \u001b[38;5;129m@acquire_spill_lock\u001b[39m()\n\u001b[1;32m   6214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mngrams_tokenize\u001b[39m(\n\u001b[1;32m   6215\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6218\u001b[0m     separator: plc\u001b[38;5;241m.\u001b[39mScalar,\n\u001b[1;32m   6219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   6220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_pylibcudf(  \u001b[38;5;66;03m# type: ignore[return-value]\u001b[39;00m\n\u001b[0;32m-> 6221\u001b[0m         \u001b[43mplc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnvtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngrams_tokenize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngrams_tokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6222\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylibcudf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mngrams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6224\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6225\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   6226\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6227\u001b[0m     )\n",
      "File \u001b[0;32mngrams_tokenize.pyx:17\u001b[0m, in \u001b[0;36mpylibcudf.nvtext.ngrams_tokenize.ngrams_tokenize\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mngrams_tokenize.pyx:49\u001b[0m, in \u001b[0;36mpylibcudf.nvtext.ngrams_tokenize.ngrams_tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /home/SaiKashyap/.conda/envs/llama_factory/include/rmm/mr/device/cuda_memory_resource.hpp"
     ]
    }
   ],
   "source": [
    "cu_vectorizer = cuTfidfVectorizer(max_features=50_000, sublinear_tf=True, ngram_range=(1, 2), min_df=2, max_df=0.95)\n",
    "X_cu_tfidf_direct = cu_vectorizer.fit_transform(gpu_df['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3ee72",
   "metadata": {},
   "source": [
    "# CPU Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b2eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_search(query_text, top_n=5):\n",
    "    query_vec = vectorizer.transform([query_text])\n",
    "    similarities = cosine_similarity(query_vec, X_tfidf_direct).flatten()\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    return df.iloc[top_indices][['English', 'text']], similarities[top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391cf02",
   "metadata": {},
   "source": [
    "# GPU Search (CuPy + cuML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b0d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_search(query_text, top_n=5):\n",
    "    query_series = cudf.Series([query_text])\n",
    "    query_vec = cu_vectorizer.transform(query_series).astype(cp.float32)\n",
    "    \n",
    "    from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix\n",
    "    query_csr = query_vec.tocsr()\n",
    "    corpus_csr = X_cu_tfidf_direct.tocsr()\n",
    "    \n",
    "    query_gpu = cp_csr_matrix(query_csr)\n",
    "    corpus_gpu = cp_csr_matrix(corpus_csr)\n",
    "\n",
    "    similarities = (query_gpu * corpus_gpu.T).todense().ravel()\n",
    "    top_indices = cp.argsort(-similarities)[:top_n].get()\n",
    "\n",
    "    return gpu_df.iloc[top_indices][['English', 'text']].to_pandas(), similarities[top_indices].get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff794524",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.preprocessing import normalize\n",
    "\n",
    "def gpu_search(query_text, top_n=5):\n",
    "    X_normalized = normalize(X_cu_tfidf_direct, norm='l2')\n",
    "    \n",
    "    nn_model = NearestNeighbors(n_neighbors=top_n, metric='cosine')\n",
    "    nn_model.fit(X_normalized)\n",
    "    \n",
    "    query_vec = normalize(cu_vectorizer.transform(cudf.Series([query_text])), norm='l2')\n",
    "    \n",
    "    distances, indices = nn_model.kneighbors(query_vec)\n",
    "    \n",
    "    return (\n",
    "        gpu_df.iloc[indices[0].get()][['English', 'text']].to_pandas(),\n",
    "        1 - distances[0].get(),  # Convert cosine distance to similarity\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff2e9a1",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61dc3349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "Operation                 CPU Time (s)    GPU Time (s)    Speedup   \n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpu_load_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOperation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCPU Time (s)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU Time (s)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeedup\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m65\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Loading\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_load_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_load_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_load_time\u001b[38;5;241m/\u001b[39mgpu_load_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTfidfVectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_tfidf_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_tfidf_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_tfidf_time\u001b[38;5;241m/\u001b[39mgpu_tfidf_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<25\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_search_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgpu_search_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<15.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_search_time\u001b[38;5;241m/\u001b[39mgpu_search_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gpu_load_time' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "print(f\"{'Operation':<25} {'CPU Time (s)':<15} {'GPU Time (s)':<15} {'Speedup':<10}\")\n",
    "print(f\"{'-'*65}\")\n",
    "print(f\"{'Data Loading':<25} {cpu_load_time:<15.4f} {gpu_load_time:<15.4f} {cpu_load_time/gpu_load_time:.2f}x\")\n",
    "print(f\"{'TfidfVectorizer':<25} {cpu_tfidf_time:<15.4f} {gpu_tfidf_time:<15.4f} {cpu_tfidf_time/gpu_tfidf_time:.2f}x\")\n",
    "print(f\"{'Search':<25} {cpu_search_time:<15.6f} {gpu_search_time:<15.6f} {cpu_search_time/gpu_search_time:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff6932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a61cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd14b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a4aeba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-llama_factory] *",
   "language": "python",
   "name": "conda-env-.conda-llama_factory-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
